{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from  keras.utils import np_utils\n",
    "from matplotlib import pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def federated_train_images(node, data, label, size, file='renameTHIS.h'):\n",
    "    # test\n",
    "    with open(file, 'w') as f:\n",
    "        num_of_image = size\n",
    "        for i in range(num_of_image):\n",
    "            f.write('#define NODE_%d_TRAIN_IMG_%d {'%(node,i) )\n",
    "            np.round(data[i]).flatten().tofile(f, sep=\", \", format=\"%d\") # convert 0~1 to 0~127\n",
    "            f.write('} \\n')\n",
    "            f.write('#define NODE_%d_TRAIN_IMG%d_LABEL'% (node, i))\n",
    "            f.write(' %d \\n \\n' % label[i])\n",
    "        f.write('#define NODE_%d_TOTAL_TRAIN_IMAGES %d \\n \\n'%(node,num_of_image))\n",
    "\n",
    "        f.write('static q7_t NODE_%d_TRAIN_IMAGES[%d][%d] = {' % (node,num_of_image, data[0].flatten().shape[0]))\n",
    "        f.write('NODE_%d_TRAIN_IMG_0'%(node))\n",
    "        for i in range(num_of_image -1):\n",
    "            f.write(',NODE_%d_TRAIN_IMG_%d'%(node, i+1))\n",
    "        f.write('};\\n\\n')\n",
    "\n",
    "        f.write('static q7_t NODE_%d_TRAIN_LABELS[%d] = {' % (node,num_of_image))\n",
    "        f.write('NODE_%d_TRAIN_IMG0_LABEL'%(node))\n",
    "        for i in range(num_of_image -1):\n",
    "            f.write(',NODE_%d_TRAIN_IMG%d_LABEL'%(node, i+1))\n",
    "        f.write('};\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imagesTestcfile(data, label, size, file='renameTHIS.h'):\n",
    "    # test\n",
    "    with open(file, 'w') as f:\n",
    "        num_of_image = size\n",
    "        print(num_of_image)\n",
    "        for i in range(num_of_image):\n",
    "#             selected = np.random.randint(0, 10000) # select 10 out of 1000.\n",
    "            f.write('#define TEST_IMG%d {'% (i))\n",
    "            np.round(data[i]).flatten().tofile(f, sep=\", \", format=\"%d\") # convert 0~1 to 0~127\n",
    "            f.write('} \\n')\n",
    "            f.write('#define TEST_IMG%d_LABEL'% (i))\n",
    "            f.write(' %d \\n \\n' % label[i])\n",
    "        f.write('#define TOTAL_TEST_IMAGES %d \\n \\n'%(num_of_image))\n",
    "\n",
    "        f.write('static q7_t TEST_IMAGES[%d][%d] = {' % (num_of_image, data[0].flatten().shape[0]))\n",
    "        f.write('TEST_IMG0')\n",
    "        for i in range(num_of_image -1):\n",
    "            f.write(',TEST_IMG%d'%(i+1))\n",
    "        f.write('};\\n\\n')\n",
    "\n",
    "        f.write('static q7_t TEST_LABELS[%d] = {' % (num_of_image))\n",
    "        f.write('TEST_IMG0_LABEL')\n",
    "        for i in range(num_of_image -1):\n",
    "            f.write(',TEST_IMG%d_LABEL'%(i+1))\n",
    "        f.write('};\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linnaeus images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3200 files belonging to 4 classes.\n",
      "Using 2560 files for training.\n",
      "Found 3200 files belonging to 4 classes.\n",
      "Using 2560 files for training.\n",
      "256\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "img_height = 32\n",
    "img_width = 32\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  \"../../Linnaeus32X32/train\",\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  \"../../Linnaeus32X32/test\",\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "# class_names = train_ds.class_names\n",
    "# print(class_names)\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "\n",
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "\n",
    "\n",
    "# image_batch, labels_batch = next(iter(normalized_ds))\n",
    "\n",
    "# x_train = image_batch.numpy()\n",
    "# y_train = labels_batch.numpy()\n",
    "\n",
    "# image_to_cfile(x_train*127, y_train, batch_size, file='FedTrainSet.h')\n",
    "\n",
    "# first_image = image_batch[0]\n",
    "# Notice the pixels values are now in `[0,1]`.\n",
    "# print(np.min(first_image), np.max(first_image))\n",
    "\n",
    "node=0\n",
    "\n",
    "for image_batch,labels_batch in normalized_ds:\n",
    "    break\n",
    "    x_train = image_batch.numpy()\n",
    "    y_train = labels_batch.numpy()\n",
    "    federated_train_images(node, x_train*127, y_train, batch_size, file='node_'+str(node)+'_images.h')\n",
    "    node +=1\n",
    "    if node == 1:\n",
    "         break\n",
    "\n",
    "image_batch, labels_batch = next(iter(normalized_ds))           \n",
    "x_test = image_batch.numpy()\n",
    "# print(y_test)\n",
    "y_test = labels_batch.numpy()\n",
    "\n",
    "\n",
    "# imagesTestcfile(x_test*127, y_test, batch_size, file='Nodes_TestSet.h')\n",
    "\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3200 files belonging to 4 classes.\n",
      "Using 2560 files for training.\n",
      "Found 3200 files belonging to 4 classes.\n",
      "Using 2560 files for training.\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_60 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "conv2d_61 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_62 (Conv2D)           (None, 16, 16, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_63 (Conv2D)           (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 64)                131136    \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 197,828\n",
      "Trainable params: 197,828\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  \"../../Linnaeus32X32/train\",\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  \"../../Linnaeus32X32/test\",\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "normalized_test = test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "\n",
    "\n",
    "dnnsplit = tf.keras.Sequential()\n",
    "#conv series 1 x2 \n",
    "dnnsplit.add(tf.keras.layers.Conv2D(filters=64,input_shape=(32,32,3),kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "dnnsplit.add(tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "dnnsplit.add(tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "#conv series 2 x2\n",
    "dnnsplit.add(tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "dnnsplit.add(tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "dnnsplit.add(tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "\n",
    "\n",
    "#Linear\n",
    "dnnsplit.add(tf.keras.layers.Flatten())\n",
    "dnnsplit.add(tf.keras.layers.Dense(64, activation='relu') ) \n",
    "dnnsplit.add(tf.keras.layers.Dense(4, activation='softmax') )\n",
    "dnnsplit.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "dnnsplit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "80/80 [==============================] - 9s 113ms/step - loss: 1.2378 - accuracy: 0.3938\n",
      "Epoch 2/20\n",
      "80/80 [==============================] - 10s 131ms/step - loss: 1.1162 - accuracy: 0.4500\n",
      "Epoch 3/20\n",
      "80/80 [==============================] - 13s 168ms/step - loss: 1.0364 - accuracy: 0.5191\n",
      "Epoch 4/20\n",
      "80/80 [==============================] - 11s 136ms/step - loss: 0.9560 - accuracy: 0.5520\n",
      "Epoch 5/20\n",
      "80/80 [==============================] - 10s 120ms/step - loss: 0.8810 - accuracy: 0.5977\n",
      "Epoch 6/20\n",
      "80/80 [==============================] - 11s 133ms/step - loss: 0.8320 - accuracy: 0.6227\n",
      "Epoch 7/20\n",
      "80/80 [==============================] - 10s 127ms/step - loss: 0.7609 - accuracy: 0.6730\n",
      "Epoch 8/20\n",
      "80/80 [==============================] - 9s 116ms/step - loss: 0.7067 - accuracy: 0.6848\n",
      "Epoch 9/20\n",
      "80/80 [==============================] - 10s 123ms/step - loss: 0.6622 - accuracy: 0.7141\n",
      "Epoch 10/20\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 0.5987 - accuracy: 0.7504\n",
      "Epoch 11/20\n",
      "80/80 [==============================] - 10s 125ms/step - loss: 0.5542 - accuracy: 0.7738\n",
      "Epoch 12/20\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 0.4621 - accuracy: 0.8098\n",
      "Epoch 13/20\n",
      "80/80 [==============================] - 10s 119ms/step - loss: 0.3642 - accuracy: 0.8586\n",
      "Epoch 14/20\n",
      "80/80 [==============================] - 9s 111ms/step - loss: 0.3083 - accuracy: 0.8828\n",
      "Epoch 15/20\n",
      "80/80 [==============================] - 9s 108ms/step - loss: 0.2516 - accuracy: 0.9043\n",
      "Epoch 16/20\n",
      "80/80 [==============================] - 9s 117ms/step - loss: 0.2116 - accuracy: 0.9164\n",
      "Epoch 17/20\n",
      "80/80 [==============================] - 10s 124ms/step - loss: 0.2085 - accuracy: 0.9277\n",
      "Epoch 18/20\n",
      "80/80 [==============================] - 11s 140ms/step - loss: 0.1717 - accuracy: 0.9352\n",
      "Epoch 19/20\n",
      "80/80 [==============================] - 13s 161ms/step - loss: 0.1066 - accuracy: 0.9676\n",
      "Epoch 20/20\n",
      "80/80 [==============================] - 11s 140ms/step - loss: 0.0601 - accuracy: 0.9797\n"
     ]
    }
   ],
   "source": [
    "history = dnnsplit.fit(normalized_ds,  epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 3s 41ms/step - loss: 2.8730 - accuracy: 0.5977\n"
     ]
    }
   ],
   "source": [
    "scores = dnnsplit.evaluate(normalized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_cfile(x_train*127, y_train, batch_size, file='c2images.h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 files belonging to 2 classes.\n",
      "Using 160 files for validation.\n",
      "['bird', 'dog']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "image_to_cfile() missing 3 required positional arguments: 'data', 'label', and 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-8a0a05d01a5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Notice the pixels values are now in `[0,1]`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# print(np.min(first_image), np.max(first_image))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mimage_to_cfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: image_to_cfile() missing 3 required positional arguments: 'data', 'label', and 'size'"
     ]
    }
   ],
   "source": [
    "batch_size = 160\n",
    "img_height = 32\n",
    "img_width = 32\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  \"Linnaeus32X32/test\",\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "test_names = test_ds.class_names\n",
    "print(test_names)\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "\n",
    "normalized_test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_test_ds))\n",
    "# first_image = image_batch[0]\n",
    "# Notice the pixels values are now in `[0,1]`.\n",
    "# print(np.min(first_image), np.max(first_image))\n",
    "\n",
    "\n",
    "\n",
    "x_train = image_batch.numpy()\n",
    "y_train = labels_batch.numpy()\n",
    "\n",
    "imagesTestcfile(x_train*127, y_train, batch_size, file='2cTestSet.h')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_cfile(data, label, size, file='renameTHIS.h'):\n",
    "    # test\n",
    "    with open(file, 'w') as f:\n",
    "        num_of_image = size\n",
    "        print(num_of_image)\n",
    "        for i in range(num_of_image):\n",
    "#             selected = np.random.randint(0, 10000) # select 10 out of 1000.\n",
    "            f.write('#define TRAIN_IMG%d {'% (i))\n",
    "            np.round(data[i]).flatten().tofile(f, sep=\", \", format=\"%d\") # convert 0~1 to 0~127\n",
    "            f.write('} \\n')\n",
    "            f.write('#define TRAIN_IMG%d_LABEL'% (i))\n",
    "            f.write(' %d \\n \\n' % label[i])\n",
    "        f.write('#define TOTAL_TRAIN_IMAGES %d \\n \\n'%(num_of_image))\n",
    "\n",
    "        f.write('static q7_t TRAIN_IMAGES[%d][%d] = {' % (num_of_image, data[0].flatten().shape[0]))\n",
    "        f.write('TRAIN_IMG0')\n",
    "        for i in range(num_of_image -1):\n",
    "            f.write(',TRAIN_IMG%d'%(i+1))\n",
    "        f.write('};\\n\\n')\n",
    "\n",
    "        f.write('static q7_t TRAIN_LABELS[%d] = {' % (num_of_image))\n",
    "        f.write('TRAIN_IMG0_LABEL')\n",
    "        for i in range(num_of_image -1):\n",
    "            f.write(',TRAIN_IMG%d_LABEL'%(i+1))\n",
    "        f.write('};\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cifar10 data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # loading the dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# # # building the input vector from the 32x32 pixels\n",
    "X_train = X_train.reshape(X_train.shape[0], 32, 32, 3)\n",
    "X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# # normalizing the data to help with the training\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# # one-hot encoding using keras' numpy-related utilities\n",
    "# n_classes = 10\n",
    "# print(\"Shape before one-hot encoding: \", y_train.shape)\n",
    "# Y_train = np_utils.to_categorical(y_train, n_classes)\n",
    "# Y_test = np_utils.to_categorical(y_test, n_classes)\n",
    "# print(\"Shape after one-hot encoding: \", Y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "!open ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "image_to_cfile(X_test*127, y_test, 100, file='cifar-100-images.h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!open ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Add a channels dimension\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_cfile(x_test*127, y_test, x_test.shape[0], file='MnistDigits.h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emnist import list_datasets\n",
    "from emnist import extract_training_samples\n",
    "from emnist import extract_test_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['balanced', 'byclass', 'bymerge', 'digits', 'letters', 'mnist']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124800, 28, 28, 1)\n",
      "(20800, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train, y_train = extract_training_samples('letters')\n",
    "x_test, y_test = extract_test_samples('letters')\n",
    "\n",
    "#normalize data \n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "#Display a random image\n",
    "plt.imshow(x_test[1])\n",
    "plt.show\n",
    "\n",
    "# Add a channels dimension\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]\n",
    "\n",
    "print (x_train.shape)\n",
    "print (x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = y_test - 1\n",
    "y_train = y_train -1\n",
    "max(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train, 26)\n",
    "y_test = np_utils.to_categorical(y_test, 26)\n",
    "\n",
    "# train_labels = np_utils.to_categorical(y_train, 27)\n",
    "# test_labels = np_utils.to_categorical(y_test, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_24 (Conv2D)           (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 28, 28, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 14, 14, 16)        2320      \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 14, 14, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 26)                1690      \n",
      "=================================================================\n",
      "Total params: 59,050\n",
      "Trainable params: 59,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dnnsplit = tf.keras.Sequential()\n",
    "#conv series 1 x2 \n",
    "dnnsplit.add(tf.keras.layers.Conv2D(filters=16,input_shape=(28,28,1),kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "dnnsplit.add(tf.keras.layers.Conv2D(filters=16,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "dnnsplit.add(tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "#conv series 2 x2\n",
    "dnnsplit.add(tf.keras.layers.Conv2D(filters=16,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "dnnsplit.add(tf.keras.layers.Conv2D(filters=16,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "dnnsplit.add(tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "\n",
    "\n",
    "#Linear\n",
    "dnnsplit.add(tf.keras.layers.Flatten())\n",
    "dnnsplit.add(tf.keras.layers.Dense(64, activation='relu') ) \n",
    "dnnsplit.add(tf.keras.layers.Dense(26, activation='softmax') )\n",
    "# model = tf.keras.Sequential()\n",
    "# model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "# model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "# model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(units=4096,activation=\"relu\"))\n",
    "# model.add(Dense(units=4096,activation=\"relu\"))\n",
    "# model.add(Dense(units=2, activation=\"softmax\"))\n",
    "\n",
    "dnnsplit.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "dnnsplit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "975/975 [==============================] - 110s 112ms/step - loss: 0.6501 - accuracy: 0.8007 - val_loss: 0.3351 - val_accuracy: 0.8938\n",
      "Epoch 2/8\n",
      "975/975 [==============================] - 90s 92ms/step - loss: 0.2934 - accuracy: 0.9056 - val_loss: 0.2713 - val_accuracy: 0.9138\n",
      "Epoch 3/8\n",
      "975/975 [==============================] - 86s 88ms/step - loss: 0.2397 - accuracy: 0.9213 - val_loss: 0.2483 - val_accuracy: 0.9218\n",
      "Epoch 4/8\n",
      "975/975 [==============================] - 90s 93ms/step - loss: 0.2109 - accuracy: 0.9298 - val_loss: 0.2373 - val_accuracy: 0.9218\n",
      "Epoch 5/8\n",
      "975/975 [==============================] - 88s 90ms/step - loss: 0.1917 - accuracy: 0.9350 - val_loss: 0.2128 - val_accuracy: 0.9285\n",
      "Epoch 6/8\n",
      "975/975 [==============================] - 88s 90ms/step - loss: 0.1768 - accuracy: 0.9396 - val_loss: 0.2003 - val_accuracy: 0.9351\n",
      "Epoch 7/8\n",
      "975/975 [==============================] - 89s 91ms/step - loss: 0.1664 - accuracy: 0.9424 - val_loss: 0.2005 - val_accuracy: 0.9354\n",
      "Epoch 8/8\n",
      "975/975 [==============================] - 88s 90ms/step - loss: 0.1549 - accuracy: 0.9457 - val_loss: 0.1916 - val_accuracy: 0.9368\n"
     ]
    }
   ],
   "source": [
    "history = dnnsplit.fit(x_train, y_train, batch_size=128, epochs=8, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650/650 [==============================] - 4s 6ms/step - loss: 0.1916 - accuracy: 0.9368\n"
     ]
    }
   ],
   "source": [
    "scores = dnnsplit.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \t# evaluate the model\n",
    "# \tscores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "# \tprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "# \tcvscores.append(scores[1] * 100)\n",
    "# print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
